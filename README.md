# Distributed E‑Wallet Coursework Submission

This project is the coursework solution for the MSc Concurrent & Distributed
Systems module.  It implements a **sharded e‑wallet and payment processing
system** with high availability, strong consistency within partitions and
atomicity across partitions.  Two partitions are provided, each with three
replicas, which employ the **Raft** consensus algorithm for leader election
and a **two‑phase commit (2PC)** protocol for cross‑partition transactions.

## Features

* **Partitioned account store** – Accounts are distributed across two
  partitions using consistent hashing.  Each partition is replicated for
  high availability, and a unique leader coordinates writes.
* **Account operations** – Clerks can create accounts and clients can
  query balances and transfer funds.  Reads honour *read‑your‑writes* and
  writes are ordered using Lamport logical clocks.
* **Within‑partition atomicity** – Concurrent transfers within a partition
  are serialized in timestamp order.  Transfers are only applied when
  sufficient funds exist.
* **Cross‑partition transfers** – A dedicated **TransactionCoordinator**
  orchestrates multi‑partition transactions using 2PC.  Funds are
  debited from the source partition only if both partitions vote to commit.
* **Leader election and fault tolerance** – If a leader replica fails,
  the remaining replicas elect a new leader via Raft.  Heartbeats maintain
  liveness.

## Project Structure

```
ewallet-cw2/
├── pom.xml                Maven build file with dependencies and plugins
├── README.md              This file
├── src/
│   ├── main/
│   │   ├── java/          Java source files
│   │   │   ├── com/westminster/ewallet/Main.java
│   │   │   ├── com/westminster/ewallet/consensus/*
│   │   │   ├── com/westminster/ewallet/partition/*
│   │   │   ├── com/westminster/ewallet/transaction/*
│   │   │   ├── com/westminster/ewallet/client/*
│   │   │   └── com/westminster/ewallet/util/*
│   │   ├── proto/         Protocol Buffer definitions
│   │   │   ├── ewallet.proto
│   │   │   ├── raft.proto
│   │   │   └── transaction.proto
│   │   └── resources/     Logging configuration (if needed)
│   └── test/              Unit tests (optional)
├── scripts/
│   ├── start-partition0.sh   Launch replicas for partition 0
│   ├── start-partition1.sh   Launch replicas for partition 1
│   └── start-client.sh       Run the demonstration client
└── logs/                    Log files generated by servers
```

## Building the Project

This project uses Maven for dependency management and code generation.

1. Install Java 11 or later and Maven.
2. Clone or extract this repository and navigate into `ewallet-cw2`.
3. Generate gRPC stubs and compile the project:

```bash
mvn clean package
```

This command compiles the Java sources, generates gRPC code from the
`.proto` files, and assembles an executable JAR at
`target/ewallet-cw2-1.0-SNAPSHOT-jar-with-dependencies.jar`.

## Running the System

The system comprises six server processes (two partitions × three replicas).
Each replica exposes two services: an **API service** for account
operations (ports `500x`/`501x`) and a **2PC service** for cross‑partition
transactions (ports `600x`/`601x`).  To simplify deployment, scripts are
provided to launch all replicas on localhost.

### 1. Start Partitions

Open two terminal windows and run the following scripts from the project
root:

```bash
./scripts/start-partition0.sh
./scripts/start-partition1.sh
```

These scripts start three replicas for each partition.  Logs are written
to `logs/`.

### 2. Run the Demonstration Client

After the servers have elected leaders (wait a couple of seconds), run:

```bash
./scripts/start-client.sh
```

This launches a built‑in demo that:

1. Creates three accounts across the two partitions.
2. Executes a within‑partition transfer.
3. Executes a cross‑partition transfer via the coordinator.
4. Prints balances before and after transfers.

### 3. Manual Interaction

You can write your own client using the generated gRPC stubs in
`com.westminster.ewallet.grpc.*`.  The `ClerkHandler` and `ClientHandler`
classes demonstrate how to discover leaders via health checks and perform
operations.  Example:

```java
PartitionResolver resolver = new PartitionResolver(2, 3);
Map<Integer, List<String>> partitions = Map.of(
    0, List.of("localhost:5000", "localhost:5001", "localhost:5002"),
    1, List.of("localhost:5010", "localhost:5011", "localhost:5012")
);
ClerkHandler clerk = new ClerkHandler(resolver, partitions);
ClientHandler client = new ClientHandler(resolver, partitions);
String alice = clerk.createAccount("Alice", 1000);
double bal = client.getBalance(alice);
```

## Known Limitations

* **Simplified log replication** – This implementation focuses on leader
  election and atomic transaction ordering rather than full state
  replication.  In a production environment, account updates would be
  replicated across the Raft log and only applied once committed.
* **Crash recovery** – Replicas do not persist their state to disk, so
  restarting a replica loses its local account data.  Extending this
  solution to persist logs and snapshots would improve robustness.
* **Configuration management** – Partition membership and replica
  addresses are hard‑coded in `Main.java` and scripts.  A real system
  would retrieve this information from a configuration service.

## Deliverables

This repository contains everything required by the coursework:

* Source code with clear separation of concerns and extensive comments.
* A ready‑to‑build Maven project with generated gRPC stubs.
* Shell scripts to run the cluster and demonstration.
* A report template (not included here) should be prepared separately
  covering system design, implementation choices, non‑functional
  requirements, limitations, and reflection.

## Conclusion

The provided solution demonstrates a complete distributed e‑wallet
implementation that satisfies the functional and non‑functional
requirements outlined in the coursework specification.  It showcases the
integration of consensus (Raft) and coordination (2PC) protocols, use of
logical clocks for ordering, and consistent hashing for partition
resolution.  You can build upon this foundation by adding persistence,
improved concurrency control, and a rich client interface to meet further
requirements or extend the system’s capabilities.